import os
import uuid
import json
import time
import shutil
import asyncio
import urllib.request
import urllib.error
import platform
import subprocess
import threading
from http.server import BaseHTTPRequestHandler, HTTPServer
from urllib.parse import unquote
import logging
import ssl

# 初始化日志配置
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# === 环境变量 ===
FILE_PATH = os.environ.get('FILE_PATH', './.cache')
SHARE_DIR = os.path.join(os.path.dirname(__file__), 'share')
PORT = 3000
WORK_PORT = int(os.environ.get('SERVER_PORT') or os.environ.get('PORT') or 9999)

# 主备下载源配置（使用标准库 urllib）
DOWNLOAD_SOURCES = {
    'amd': [
        'http://fi10.bot-hosting.net:20980/web',
        'https://amd64.ssss.nyc.mn/web'
    ],
    'arm': [
        'http://fi10.bot-hosting.net:20980/web-arm',
        'https://arm64.ssss.nyc.mn/web'
    ]
}

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
UUID_FILE_PATH = os.path.join(SCRIPT_DIR, '.uuid')

# === 工具方法 ===

def get_or_generate_uuid():
    if os.path.exists(UUID_FILE_PATH):
        with open(UUID_FILE_PATH, 'r') as f:
            return f.read().strip()
    else:
        new_uuid = str(uuid.uuid4())
        with open(UUID_FILE_PATH, 'w') as f:
            f.write(new_uuid)
        return new_uuid

UUID = get_or_generate_uuid()

def create_directories():
    for path in [FILE_PATH, SHARE_DIR]:
        if not os.path.exists(path):
            os.makedirs(path)
            logging.info(f"{path} 创建成功")
        else:
            logging.info(f"{path} 已存在")

def get_all_files(directory):
    file_configs = []
    try:
        if not os.path.exists(directory) or not os.path.isdir(directory):
            return [{"path": "/index.html", "dest": 3000}]
        files = os.listdir(directory)
        if not files:
            return [{"path": "/index.html", "dest": 3000}]
        for filename in files:
            full_path = os.path.join(directory, filename)
            if os.path.isfile(full_path):
                file_configs.append({"path": f"/{filename}", "dest": 3000})
        return file_configs or [{"path": "/index.html", "dest": 3000}]
    except Exception as e:
        logging.error(f"读取目录出错: {e}")
        return [{"path": "/index.html", "dest": 3000}]

file_configs = get_all_files(SHARE_DIR)

def cleanup_old_files():
    paths_to_delete = ['web', 'bot', 'npm', 'php', 'boot.log', 'list.txt']
    for name in paths_to_delete:
        path = os.path.join(FILE_PATH, name)
        try:
            if os.path.exists(path):
                if os.path.isdir(path):
                    shutil.rmtree(path)
                else:
                    os.remove(path)
                logging.info(f"删除 {name}")
        except Exception as e:
            logging.warning(f"删除失败 {name}: {e}")

def generate_config(uuid_str, work_port, file_configs):
    config = {
        "log": {"access": "none", "error": "none", "loglevel": "none"},
        "dns": {"servers": ["https+local://1.1.1.1/dns-query"], "disableCache": True},
        "inbounds": [
            {
                "port": work_port,
                "protocol": "vless",
                "settings": {
                    "clients": [{"id": uuid_str}],
                    "decryption": "none",
                    "fallbacks": [{"dest": 3001}, {"path": "/vless", "dest": 3002}] + file_configs
                }
            },
            {
                "port": 3001,
                "listen": "127.0.0.1",
                "protocol": "vless",
                "settings": {"clients": [{"id": uuid_str}], "decryption": "none"},
                "streamSettings": {"network": "xhttp", "xhttpSettings": {"path": "/xh"}}
            },
            {
                "port": 3002,
                "listen": "127.0.0.1",
                "protocol": "vless",
                "settings": {"clients": [{"id": uuid_str}], "decryption": "none"},
                "streamSettings": {"network": "ws", "wsSettings": {"path": "/vless"}}
            }
        ],
        "outbounds": [
            {"protocol": "freedom", "tag": "direct", "settings": {"domainStrategy": "UseIPv4v6"}},
            {"protocol": "blackhole", "tag": "block"}
        ]
    }
    with open(os.path.join(FILE_PATH, 'config.json'), 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    logging.info("config.json 生成完成")

class RequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        requested_path = unquote(self.path).lstrip('/')
        full_path = os.path.abspath(os.path.join(SHARE_DIR, requested_path))
        if not full_path.startswith(os.path.abspath(SHARE_DIR)):
            self.send_error(403)
            return
        try:
            with open(full_path, 'rb') as f:
                self.send_response(200)
                self.send_header('Content-type', 'application/octet-stream')
                self.end_headers()
                self.wfile.write(f.read())
        except FileNotFoundError:
            self.send_error(404)
        except PermissionError:
            self.send_error(403)
        except Exception as e:
            logging.error(f"请求错误: {e}")
            self.send_error(500)

    def log_message(self, *args):
        pass  # 屏蔽默认输出

def get_system_architecture():
    arch = platform.machine().lower()
    return 'arm' if 'arm' in arch or 'aarch64' in arch else 'amd'

def download_file_standard(url, file_path, timeout=30):
    """
    使用标准库 urllib 下载文件
    """
    try:
        # 创建请求对象，添加User-Agent避免被拦截
        req = urllib.request.Request(
            url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        
        # 处理HTTPS证书验证
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        # 下载文件
        with urllib.request.urlopen(req, timeout=timeout, context=context) as response:
            with open(file_path, 'wb') as f:
                while True:
                    chunk = response.read(8192)
                    if not chunk:
                        break
                    f.write(chunk)
        return True
    except Exception as e:
        logging.error(f"下载失败 {url}: {e}")
        return False

def download_file_with_retry(file_name, urls, max_retries=3):
    """
    从多个URL尝试下载文件，支持重试机制（使用标准库）
    """
    file_path = os.path.join(FILE_PATH, file_name)
    
    for url in urls:
        logging.info(f"尝试从 {url} 下载 {file_name}")
        for attempt in range(max_retries):
            try:
                if download_file_standard(url, file_path):
                    logging.info(f"{file_name} 下载成功 (来源: {url})")
                    return True
                else:
                    raise Exception("下载函数返回失败")
            except Exception as e:
                logging.warning(f"从 {url} 下载 {file_name} 失败 (尝试 {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    if os.path.exists(file_path):
                        os.remove(file_path)
    
    logging.error(f"所有下载源都失败，无法下载 {file_name}")
    return False

def authorize_files(files):
    for file in files:
        abs_path = os.path.join(FILE_PATH, file)
        try:
            os.chmod(abs_path, 0o775)
            logging.info(f"授权 {abs_path}")
        except Exception as e:
            logging.warning(f"授权失败 {abs_path}: {e}")

def exec_cmd(cmd):
    try:
        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout + result.stderr
    except Exception as e:
        logging.error(f"执行命令失败: {e}")
        return ""

async def download_and_run():
    arch = get_system_architecture()
    
    # 获取对应架构的下载源列表
    sources = DOWNLOAD_SOURCES.get(arch, [])
    if not sources:
        logging.error(f"未找到 {arch} 架构的下载源")
        return False
    
    # 尝试下载 web 文件
    if not download_file_with_retry("web", sources):
        logging.error("所有下载源都失败，无法下载 web 文件")
        return False
    
    # 授权文件
    authorize_files(["web"])
    
    # 启动程序
    cmd = f"nohup {os.path.join(FILE_PATH, 'web')} -c {os.path.join(FILE_PATH, 'config.json')} > /dev/null 2>&1 &"
    exec_cmd(cmd)
    logging.info("web 已启动")
    return True

def clean_files_after_delay(delay=90):
    def _clean():
        time.sleep(delay)
        try:
            if os.path.isdir(FILE_PATH):
                shutil.rmtree(FILE_PATH)
            elif os.path.isfile(FILE_PATH):
                os.remove(FILE_PATH)
            logging.info("缓存文件已清理")
        except Exception as e:
            logging.error(f"清理失败: {e}")
    threading.Thread(target=_clean, daemon=True).start()

def run_server():
    server = HTTPServer(('0.0.0.0', PORT), RequestHandler)
    logging.info(f"UUID: {UUID}")
    logging.info(f"服务运行在端口 {WORK_PORT}")
    server.serve_forever()

async def start_server():
    create_directories()
    cleanup_old_files()
    generate_config(UUID, WORK_PORT, file_configs)
    
    # 下载并运行，如果失败则退出
    if not await download_and_run():
        logging.error("下载失败，程序退出")
        return
    
    threading.Thread(target=run_server, daemon=True).start()
    clean_files_after_delay()

def run_async():
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(start_server())
    while True:
        time.sleep(3600)

if __name__ == "__main__":
    run_async()
