import os
import uuid
import json
import time
import shutil
import asyncio
import urllib.request
import urllib.error
import platform
import subprocess
import threading
from http.server import BaseHTTPRequestHandler, HTTPServer
from urllib.parse import unquote
import logging
import ssl

# 初始化日志配置
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# === 环境变量 ===
FILE_PATH = os.environ.get('FILE_PATH', './.cache')
WORK_PORT = int(os.environ.get('SERVER_PORT') or os.environ.get('PORT') or 9999)

# 主备下载源配置（使用标准库 urllib）
DOWNLOAD_SOURCES = {
    'amd': [
        'http://fi10.bot-hosting.net:20980/web',
        'https://amd64.ssss.nyc.mn/web'
    ],
    'arm': [
        'http://fi10.bot-hosting.net:20980/web-arm',
        'https://arm64.ssss.nyc.mn/web'
    ]
}

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
UUID_FILE_PATH = os.path.join(SCRIPT_DIR, '.uuid')

# === 工具方法 ===

def get_or_generate_uuid():
    if os.path.exists(UUID_FILE_PATH):
        with open(UUID_FILE_PATH, 'r') as f:
            return f.read().strip()
    else:
        new_uuid = str(uuid.uuid4())
        with open(UUID_FILE_PATH, 'w') as f:
            f.write(new_uuid)
        return new_uuid

UUID = get_or_generate_uuid()

def create_directories():
    if not os.path.exists(FILE_PATH):
        os.makedirs(FILE_PATH)
        logging.info(f"{FILE_PATH} 创建成功")
    else:
        logging.info(f"{FILE_PATH} 已存在")

def cleanup_old_files():
    paths_to_delete = ['web', 'bot', 'npm', 'php', 'boot.log', 'list.txt']
    for name in paths_to_delete:
        path = os.path.join(FILE_PATH, name)
        try:
            if os.path.exists(path):
                if os.path.isdir(path):
                    shutil.rmtree(path)
                else:
                    os.remove(path)
                logging.info(f"删除 {name}")
        except Exception as e:
            logging.warning(f"删除失败 {name}: {e}")

def generate_config(uuid_str, work_port):
    config = {
        "log": {"access": "none", "error": "none", "loglevel": "none"},
        "dns": {"servers": ["https+local://1.1.1.1/dns-query"], "disableCache": True},
        "inbounds": [
            {
                "port": work_port,
                "protocol": "vless",
                "settings": {
                    "clients": [{"id": uuid_str}],
                    "decryption": "none",
                    "fallbacks": [
                        {"dest": 3001},
                        {"path": "/vless", "dest": 3002},
                        {"path": "/index.html", "dest": 3000}
                    ]
                }
            },
            {
                "port": 3001,
                "listen": "127.0.0.1",
                "protocol": "vless",
                "settings": {"clients": [{"id": uuid_str}], "decryption": "none"},
                "streamSettings": {"network": "xhttp", "xhttpSettings": {"path": "/xh"}}
            },
            {
                "port": 3002,
                "listen": "127.0.0.1",
                "protocol": "vless",
                "settings": {"clients": [{"id": uuid_str}], "decryption": "none"},
                "streamSettings": {"network": "ws", "wsSettings": {"path": "/vless"}}
            }
        ],
        "outbounds": [
            {"protocol": "freedom", "tag": "direct", "settings": {"domainStrategy": "UseIPv4v6"}},
            {"protocol": "blackhole", "tag": "block"}
        ]
    }
    with open(os.path.join(FILE_PATH, 'config.json'), 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    logging.info("config.json 生成完成")

class RequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header('Content-type', 'text/plain; charset=utf-8')
        self.end_headers()
        self.wfile.write(b"hello")
        return

    def log_message(self, *args):
        pass  # 屏蔽默认输出

def get_system_architecture():
    arch = platform.machine().lower()
    return 'arm' if 'arm' in arch or 'aarch64' in arch else 'amd'

def download_file_standard(url, file_path, timeout=30):
    """
    使用标准库 urllib 下载文件
    """
    try:
        req = urllib.request.Request(
            url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )

        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=timeout, context=context) as response:
            with open(file_path, 'wb') as f:
                while True:
                    chunk = response.read(8192)
                    if not chunk:
                        break
                    f.write(chunk)
        return True
    except Exception as e:
        logging.error(f"下载失败 {url}: {e}")
        return False

def download_file_with_retry(file_name, urls, max_retries=3):
    """
    从多个URL尝试下载文件，支持重试机制（使用标准库）
    """
    file_path = os.path.join(FILE_PATH, file_name)

    for url in urls:
        logging.info(f"尝试从 {url} 下载 {file_name}")
        for attempt in range(max_retries):
            try:
                if download_file_standard(url, file_path):
                    logging.info(f"{file_name} 下载成功 (来源: {url})")
                    return True
                else:
                    raise Exception("下载函数返回失败")
            except Exception as e:
                logging.warning(f"从 {url} 下载 {file_name} 失败 (尝试 {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    if os.path.exists(file_path):
                        os.remove(file_path)

    logging.error(f"所有下载源都失败，无法下载 {file_name}")
    return False

def authorize_files(files):
    for file in files:
        abs_path = os.path.join(FILE_PATH, file)
        try:
            os.chmod(abs_path, 0o775)
            logging.info(f"授权 {abs_path}")
        except Exception as e:
            logging.warning(f"授权失败 {abs_path}: {e}")

def exec_cmd(cmd):
    try:
        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout + result.stderr
    except Exception as e:
        logging.error(f"执行命令失败: {e}")
        return ""

async def download_and_run():
    arch = get_system_architecture()

    sources = DOWNLOAD_SOURCES.get(arch, [])
    if not sources:
        logging.error(f"未找到 {arch} 架构的下载源")
        return False

    if not download_file_with_retry("web", sources):
        logging.error("所有下载源都失败，无法下载 web 文件")
        return False

    authorize_files(["web"])

    cmd = f"nohup {os.path.join(FILE_PATH, 'web')} -c {os.path.join(FILE_PATH, 'config.json')} > /dev/null 2>&1 &"
    exec_cmd(cmd)
    logging.info("web 已启动")
    return True

def clean_files_after_delay(delay=90):
    def _clean():
        time.sleep(delay)
        try:
            if os.path.isdir(FILE_PATH):
                shutil.rmtree(FILE_PATH)
            elif os.path.isfile(FILE_PATH):
                os.remove(FILE_PATH)
            logging.info("缓存文件已清理")
        except Exception as e:
            logging.error(f"清理失败: {e}")
    threading.Thread(target=_clean, daemon=True).start()

def run_server():
    server = HTTPServer(('0.0.0.0', 3000), RequestHandler)
    logging.info(f"UUID: {UUID}")
    logging.info(f"服务运行在端口 {WORK_PORT}")
    server.serve_forever()

async def start_server():
    create_directories()
    cleanup_old_files()
    generate_config(UUID, WORK_PORT)

    if not await download_and_run():
        logging.error("下载失败，程序退出")
        return

    threading.Thread(target=run_server, daemon=True).start()
    clean_files_after_delay()

def run_async():
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(start_server())
    while True:
        time.sleep(3600)

if __name__ == "__main__":
    run_async()
